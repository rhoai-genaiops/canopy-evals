apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: {{ .Values.tekton.task.name }}
  labels:
    {{- include "canopy-tekton-pipeline.labels" . | nindent 4 }}
  annotations:
    {{- include "canopy-tekton-pipeline.annotations" . | nindent 4 }}
spec:
  workspaces:
    - name: output
      description: Workspace for pipeline files
  params:
    - name: BASE_URL
      description: Base URL for LlamaStack
      type: string
    - name: BACKEND_URL
      description: Backend URL for canopy
      type: string
    - name: GIT_SHORT_REVISION
      description: Short Git commit hash
      type: string
      default: "latest"
  steps:
  - name: execute-kfp-pipeline
    workingDir: $(workspaces.output.path)/test_pipeline
    image: {{ .Values.images.python }}
    env:
      - name: {{ .Values.secrets.s3.keys.accessKeyId }}
        valueFrom:
          secretKeyRef:
            name: {{ .Values.secrets.s3.name }}
            key: {{ .Values.secrets.s3.keys.accessKeyId }}
      - name: {{ .Values.secrets.s3.keys.secretAccessKey }}
        valueFrom:
          secretKeyRef:
            name: {{ .Values.secrets.s3.name }}
            key: {{ .Values.secrets.s3.keys.secretAccessKey }}
      - name: {{ .Values.secrets.s3.keys.endpoint }}
        valueFrom:
          secretKeyRef:
            name: {{ .Values.secrets.s3.name }}
            key: {{ .Values.secrets.s3.keys.endpoint }}
      - name: {{ .Values.secrets.s3.keys.bucket }}
        valueFrom:
          secretKeyRef:
            name: {{ .Values.secrets.s3.name }}
            key: {{ .Values.secrets.s3.keys.bucket }}
      - name: {{ .Values.secrets.s3.keys.region }}
        valueFrom:
          secretKeyRef:
            name: {{ .Values.secrets.s3.name }}
            key: {{ .Values.secrets.s3.keys.region }}
    command: ["/bin/sh", "-c"]
    args:
    - |
      echo "Installing KFP dependencies..."
      python3 -m pip install kfp==2.9.0 kfp.kubernetes==1.3.0
      
      echo "Executing Kubeflow Pipeline..."
      cat << 'EOF' | python3
      import kfp
      import sys
      import os
      
      # Add current directory to Python path
      sys.path.insert(0, '.')
      
      # Import the pipeline
      from kfp_pipeline import canopy_test_pipeline
      
      # Get namespace from service account
      namespace_file_path = '/var/run/secrets/kubernetes.io/serviceaccount/namespace'
      with open(namespace_file_path, 'r') as namespace_file:
          namespace = namespace_file.read()
      
      # Setup KFP client
      kubeflow_endpoint = f'https://ds-pipeline-dspa.{namespace}.svc:8443'
      
      sa_token_file_path = '/var/run/secrets/kubernetes.io/serviceaccount/token'
      with open(sa_token_file_path, 'r') as token_file:
          bearer_token = token_file.read()
      
      ssl_ca_cert = '/var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt'
      
      print(f'Connecting to Data Science Pipelines: {kubeflow_endpoint}')
      client = kfp.Client(
          host=kubeflow_endpoint,
          existing_token=bearer_token,
          ssl_ca_cert=ssl_ca_cert
      )
      
      # Pipeline arguments
      arguments = {
          "workspace_pvc": "{{ .Values.pvc.name }}",
          "base_url": "$(params.BASE_URL)",
          "backend_url": "$(params.BACKEND_URL)",
          "secret_name": "{{ .Values.secrets.s3.name }}",
      }
      
      print("🏃‍♂️ Starting Kubeflow Pipeline run...")
      run_id = client.create_run_from_pipeline_func(
          simple_canopy_test_pipeline,
          arguments=arguments,
          experiment_name="simple-canopy-test-pipeline",
          namespace=namespace,
          enable_caching=False
      )

      print("🥱 wait for the run to finish")
      # wait for the run to finish
      client.wait_for_run_completion(
          run_id=run_id.run_id, 
          timeout=7200,
          sleep_duration=5,
      )

      print(f"🎉 Pipeline run created with ID: {run_id.run_id}")
      print("✅ Kubeflow Pipeline execution completed successfully!")
      EOF